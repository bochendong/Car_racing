{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpANlTAssfdF"
      },
      "source": [
        "# Colab env settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfHgya4Jsdas",
        "outputId": "63f8d38b-9a02-4a2e-c3dd-531f80ef8ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'car_racing'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 69 (delta 36), reused 18 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (69/69), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bochendong/car_racing.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX7L7IlDnyvU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo apt update && sudo apt install python-opengl\n",
        "!sudo apt update && sudo apt install xvfb\n",
        "!pip install gym-notebook-wrapper stable-baselines[mpi] box2d box2d-kengz pyvirtualdisplay pyglet\n",
        "!pip install wandb\n",
        "!pip install gym==0.21.0\n",
        "!pip install pyvirtualdisplay -qq\n",
        "!pip install folium==0.2.1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUgm5ewmnyvV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Beta\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "\n",
        "from torch.autograd import Function\n",
        "\n",
        "import gnwrapper\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(30)\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "import wandb\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x-zGeDRnyvX"
      },
      "outputs": [],
      "source": [
        "!pip install box2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAbUZ5PBnyvY"
      },
      "outputs": [],
      "source": [
        "# Use cuda if present\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device available for running: \")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ijK2eBYnyvZ"
      },
      "outputs": [],
      "source": [
        "%cd car_racing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ia5ibGfqWfZ"
      },
      "outputs": [],
      "source": [
        "import car_racing as cr\n",
        "import replay_memory as rm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ceEsiT0Mskk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "if (os.path.exists(\"./output\")) == False:\n",
        "    os.mkdir(\"output\")\n",
        "\n",
        "for epoch in range (3000):\n",
        "    files = glob.glob(\"./output/*.png\")\n",
        "\n",
        "    for f in files:\n",
        "        os.remove(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu12ch3KssCE"
      },
      "source": [
        "# Car_racing Enviorment settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bSmOb2d6qPq"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBhiAnYKnyvZ"
      },
      "outputs": [],
      "source": [
        "env_green = gnwrapper.Animation(cr.CarRacing())\n",
        "env_green = cr.CarRacing(color = 'g')\n",
        "obs = env_green.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiivK79Vnyvc"
      },
      "outputs": [],
      "source": [
        "discrete_actions = {0 : np.array([0,0,0]),       # do nothing\n",
        "                                 1 : np.array([-1,0,0]),      # steer sharp left\n",
        "                                 2 : np.array([1,0,0]),       # steer sharp right\n",
        "                                 3 : np.array([-0.5,0,0]),    # steer left\n",
        "                                 4 : np.array([0.5,0,0]),     # steer right\n",
        "                                 5 : np.array([0,1,0]),       # accelerate 100%\n",
        "                                 6 : np.array([0,0.5,0]),     # accelerate 50%\n",
        "                                 7 : np.array([0,0.25,0]),    # accelerate 25%\n",
        "                                 8 : np.array([0,0,1]),       # brake 100%\n",
        "                                 9 : np.array([0,0,0.5]),     # brake 50%\n",
        "                                 10 : np.array([0,0,0.25])}   # brake 25%\n",
        "\n",
        "def get_obs(env):\n",
        "    n_actions = len(discrete_actions)\n",
        "    for i in range (0, 40):\n",
        "        action = torch.randint(low=0, high=11, size=(1,))\n",
        "        action_transfered = discrete_actions.get(int(action[0]))\n",
        "\n",
        "        obs, reward, done, _ = env.step([action_transfered[0], action_transfered[1], action_transfered[2]])\n",
        "\n",
        "    return obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLekXvPFZEmp"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def rand_env(s):\n",
        "    red_channel = 102\n",
        "    green_channel = 102\n",
        "    blue_channel = 102\n",
        "\n",
        "    while (red_channel == 102 and green_channel == 102 and blue_channel == 102):\n",
        "        add_green = random.randint(0, 1)\n",
        "        add_red = random.randint(0, 1)\n",
        "        add_blue = random.randint(0, 1)\n",
        "\n",
        "        if (add_red):\n",
        "            red_channel = random.randint(150, 229)\n",
        "        if (add_green):\n",
        "            green_channel = random.randint(150, 229)\n",
        "        if (add_blue):\n",
        "            blue_channel = random.randint(150, 229)\n",
        "\n",
        "\n",
        "    for i in range (96):\n",
        "        for j in range (96):\n",
        "            if (s[i][j][0] == s[i][j][1] == s[i][j][2]):\n",
        "                  continue\n",
        "            elif (s[i][j][1] == 204):\n",
        "                  s[i][j][0] = red_channel\n",
        "                  s[i][j][1] = green_channel\n",
        "                  s[i][j][2] = blue_channel\n",
        "            elif (s[i][j][1] == 230):\n",
        "                  s[i][j][0] = red_channel + 26\n",
        "                  s[i][j][1] = green_channel + 26\n",
        "                  s[i][j][2] = blue_channel + 26\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzDylNjkZG55"
      },
      "outputs": [],
      "source": [
        "rand_envs = []\n",
        "test_state = get_obs(env_green)\n",
        "\n",
        "for i in range (8):\n",
        "    green_copy = test_state.copy()\n",
        "    ss = rand_env(green_copy)\n",
        "    rand_envs.append(ss)\n",
        "\n",
        "f, axs = plt.subplots(2, 4, figsize = (12, 4))\n",
        "axs = axs.flatten()\n",
        "\n",
        "for img, ax in zip(rand_envs, axs):\n",
        "    ax.imshow(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRgDy2crSv9W"
      },
      "source": [
        "# Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuIh8kXuszw6"
      },
      "outputs": [],
      "source": [
        "class Env():\n",
        "    def __init__(self, color, seed = 0):\n",
        "        self.env = gnwrapper.Animation(cr.CarRacing(color = color))\n",
        "        self.env = cr.CarRacing(color = color)\n",
        "        self.color = color\n",
        "        self.env.seed(seed)\n",
        "        self.reward_threshold = 1000\n",
        "\n",
        "    def reset(self):\n",
        "        self.counter = 0\n",
        "        self.av_r = self.reward_memory()\n",
        "\n",
        "        self.die = False\n",
        "\n",
        "        img_rgb = self.env.reset()\n",
        "        rand_img_rgb = self.rand_state(img_rgb)\n",
        "\n",
        "        img_rgb = img_rgb / 128. - 1\n",
        "        rand_img_rgb = rand_img_rgb / 128. - 1\n",
        "\n",
        "        self.stack = [img_rgb] * 4\n",
        "        self.rand_stack = [rand_img_rgb] * 4\n",
        "\n",
        "        return np.array(self.stack), np.array(self.rand_stack)\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0\n",
        "        for i in range(8):\n",
        "            img_rgb, reward, die, _ = self.env.step(action)\n",
        "\n",
        "            if die: reward += 100\n",
        "\n",
        "            if self.color == 'g' and np.mean(img_rgb[:, :, 1]) > 185.0:\n",
        "                  reward -= 0.05\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            done = True if self.av_r(reward) <= -0.1 else False\n",
        "            if done or die:\n",
        "                break\n",
        "\n",
        "        rand_img_rgb = self.rand_state(img_rgb)\n",
        "\n",
        "        img_rgb = img_rgb / 128. - 1\n",
        "        rand_img_rgb = rand_img_rgb / 128. - 1\n",
        "\n",
        "        self.stack.pop(0)\n",
        "        self.stack.append(img_rgb)\n",
        "\n",
        "        assert len(self.stack) == 4\n",
        "\n",
        "        return np.array(self.stack), total_reward, done, die\n",
        "\n",
        "    def step_eval(self, action):\n",
        "        img_rgb, reward, done, _ = self.env.step(action)\n",
        "\n",
        "        rand_img_rgb = self.rand_state(img_rgb)\n",
        "\n",
        "        rand_img_rgb = rand_img_rgb / 128. - 1\n",
        "        img_rgb = img_rgb / 128. - 1\n",
        "\n",
        "        self.stack.pop(0)\n",
        "        self.stack.append(img_rgb)\n",
        "        \n",
        "        return np.array(self.stack), reward, done, _\n",
        "\n",
        "    def render(self, *arg):\n",
        "        self.env.render(*arg)\n",
        "\n",
        "    @staticmethod\n",
        "    def reward_memory():\n",
        "        count = 0\n",
        "        length = 100\n",
        "        history = np.zeros(length)\n",
        "\n",
        "        def memory(reward):\n",
        "            nonlocal count\n",
        "            history[count] = reward\n",
        "            count = (count + 1) % length\n",
        "            return np.mean(history)\n",
        "\n",
        "        return memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIYnOK21S5cD"
      },
      "source": [
        "# DANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Qatrg357OC6"
      },
      "outputs": [],
      "source": [
        "class DANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DANN, self).__init__()\n",
        "        self.sketch = nn.Sequential(\n",
        "            nn.Conv3d(4, 4, kernel_size=(1, 1, 3), stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.feature = nn.Sequential(  # input shape (4, 96, 96)\n",
        "            nn.Conv2d(4, 8, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=2),  # (8, 47, 47)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2),  # (16, 23, 23)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2),  # (32, 11, 11)\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.cnn_base = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1),  # (64, 5, 5)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1),  # (128, 3, 3)\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.v = nn.Sequential(nn.Linear(256, 100), nn.ReLU(), nn.Linear(100, 1))\n",
        "        self.fc = nn.Sequential(nn.Linear(256, 100), nn.ReLU())\n",
        "        self.alpha_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n",
        "        self.beta_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n",
        "        self.apply(self._weights_init)\n",
        "      \n",
        "    def forward(self, input):\n",
        "        sketch = self.sketch(input)\n",
        "        sketch = torch.squeeze(sketch)\n",
        "\n",
        "        feature = self.feature(sketch)\n",
        "\n",
        "        out = self.cnn_base(feature)\n",
        "        out = out.view(-1, 256)\n",
        "        v = self.v(out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        alpha = self.alpha_head(out) + 1\n",
        "        beta = self.beta_head(out) + 1\n",
        "\n",
        "        return (alpha, beta), v\n",
        "\n",
        "    @staticmethod\n",
        "    def _weights_init(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
        "            nn.init.constant_(m.bias, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIfkUhtrbgqw"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_r8gQMUbgCP"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    max_grad_norm = 0.5\n",
        "    clip_param = 0.1\n",
        "    ppo_epoch = 10\n",
        "\n",
        "    transition = np.dtype([('s', np.float64, (4, 96, 96, 3)), ('a', np.float64, (3,)), ('a_logp', np.float64),\n",
        "                       ('r', np.float64), ('s_', np.float64, (4, 96, 96, 3))])\n",
        "\n",
        "    def __init__(self, net, criterion, optimizer, buffer_capacity = 2000, batch_size = 128):\n",
        "        self.net = net\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        self.batch_size = batch_size\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.source_buffer = np.empty(self.buffer_capacity, dtype=self.transition)\n",
        "        self.counter = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.from_numpy(state).double().to(device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            out = self.net.sketch(state)\n",
        "            out = torch.squeeze(out)\n",
        "            out = self.net.feature(out)\n",
        "\n",
        "            out = self.net.cnn_base(out)\n",
        "            out = out.view(-1, 256)\n",
        "            out = self.net.fc(out)\n",
        "            alpha = self.net.alpha_head(out) + 1\n",
        "            beta = self.net.beta_head(out) + 1\n",
        "\n",
        "        dist = Beta(alpha, beta)\n",
        "        action = dist.sample()\n",
        "        a_logp = dist.log_prob(action).sum(dim=1)\n",
        "\n",
        "        action = action.squeeze().cpu().numpy()\n",
        "        a_logp = a_logp.item()\n",
        "        return action, a_logp\n",
        "\n",
        "    def store(self, transition):\n",
        "        self.source_buffer[self.counter] = transition\n",
        "        self.counter += 1\n",
        "        if self.counter == self.buffer_capacity:\n",
        "            self.counter = 0\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def update(self):\n",
        "        s = torch.tensor(self.source_buffer['s'], dtype=torch.double).to(device)\n",
        "        a = torch.tensor(self.source_buffer['a'], dtype=torch.double).to(device)\n",
        "        r = torch.tensor(self.source_buffer['r'], dtype=torch.double).to(device).view(-1, 1)\n",
        "        s_ = torch.tensor(self.source_buffer['s_'], dtype=torch.double).to(device)\n",
        "        old_a_logp = torch.tensor(self.source_buffer['a_logp'], dtype=torch.double).to(device).view(-1, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            target_v = r + 0.99 * self.net(s_)[1]\n",
        "            adv = target_v - self.net(s)[1]\n",
        "  \n",
        "        for _ in range(self.ppo_epoch):\n",
        "            for index in BatchSampler(SubsetRandomSampler(range(self.buffer_capacity)), self.batch_size, True):\n",
        "                loss = 0\n",
        "\n",
        "                (alpha, beta), v = self.net(s[index])\n",
        "               \n",
        "                dist = Beta(alpha, beta)\n",
        "                a_logp = dist.log_prob(a[index]).sum(dim=1, keepdim=True)\n",
        "                ratio = torch.exp(a_logp - old_a_logp[index])\n",
        "                surr1 = ratio * adv[index]\n",
        "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * adv[index]\n",
        "                action_loss = -torch.min(surr1, surr2).mean()\n",
        "                value_loss = F.smooth_l1_loss(self.net(s[index])[1], target_v[index])\n",
        "\n",
        "                loss += action_loss + 2. * value_loss\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ieYDKlqQhoL"
      },
      "source": [
        "# Env training method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0cmDpQ-a1CA"
      },
      "outputs": [],
      "source": [
        "def train(source_env, agent):\n",
        "    training_records = []\n",
        "    running_score = 0\n",
        "\n",
        "    for i_ep in range(3000):\n",
        "        score = 0\n",
        "        state = source_env.reset()\n",
        "        for t in range(1000):\n",
        "            action, a_logp = agent.select_action(state)\n",
        "            state_, reward, done, die = source_env.step(action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n",
        "            score += reward\n",
        "\n",
        "            should_update = agent.store((state, action, a_logp, reward, state_))\n",
        "\n",
        "            if should_update: \n",
        "                agent.update()\n",
        "\n",
        "            state = state_\n",
        "\n",
        "            if done or die: break\n",
        "\n",
        "        training_records.append(score)\n",
        "\n",
        "        running_score = running_score * 0.99 + score * 0.01\n",
        "        if (i_ep % 10 == 0):\n",
        "            print('Ep {}\\tLast score: {:.2f}\\tMoving average score: {:.2f}'.format(i_ep, score, running_score))\n",
        "\n",
        "    return training_records\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM05fgO6WM4S"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSdWNIoGWPmD"
      },
      "source": [
        "## Green Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUwoUHmRVzD_"
      },
      "outputs": [],
      "source": [
        "green_env = Env(color = 'g', seed = 0)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "net = DANN().double().to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "agent = Agent(net = net,  criterion = criterion,  optimizer = optimizer, buffer_capacity = 3000, batch_size = 64)\n",
        "\n",
        "green_record = train(green_env, agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3F4B7G8WRwa"
      },
      "source": [
        "## Red Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlPbMqzqWfD7"
      },
      "outputs": [],
      "source": [
        "red_env = Env(color = 'r', seed = 0)\n",
        "net = DANN().double().to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "agent = Agent(net = net,  criterion = criterion,  optimizer = optimizer, buffer_capacity = 3000, batch_size = 64)\n",
        "\n",
        "red_record = train(red_env, agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clncdDuRWUpz"
      },
      "source": [
        "##Bule Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMWCNbIyWfd3"
      },
      "outputs": [],
      "source": [
        "blue_env = Env(color = 'b', seed = 0)\n",
        "net = DANN().double().to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "agent = Agent(net = net,  criterion = criterion,  optimizer = optimizer, buffer_capacity = 3000, batch_size = 64)\n",
        "\n",
        "blue_record = train(blue_env, agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfgEoIUqWVsZ"
      },
      "source": [
        "## Yellow Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oY8iUq9Wf9b"
      },
      "outputs": [],
      "source": [
        "yellow_env = Env(color = 'y', seed = 0)\n",
        "net = DANN().double().to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "agent = Agent(net = net,  criterion = criterion,  optimizer = optimizer, buffer_capacity = 3000, batch_size = 64)\n",
        "\n",
        "blue_record = train(yellow_env, agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y-QbKExWXKl"
      },
      "source": [
        "## Gray Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5B9GuLnWgdR"
      },
      "outputs": [],
      "source": [
        "gray_env = Env(color = 'gr', seed = 0)\n",
        "net = DANN().double().to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "agent = Agent(net = net,  criterion = criterion,  optimizer = optimizer, buffer_capacity = 3000, batch_size = 64)\n",
        "\n",
        "blue_record = train(gray_env, agent)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "car_racing_colab.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
