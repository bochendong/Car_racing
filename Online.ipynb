{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Beta\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "from torch.autograd import Function\n",
    "\n",
    "import gnwrapper\n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "gymlogger.set_level(30)\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "import wandb\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import car_racing as cr\n",
    "\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists(\"./output\")) == False:\n",
    "    os.mkdir(\"output\")\n",
    "    \n",
    "for epoch in range (3000):\n",
    "    files = glob.glob(\"./output/*.png\")\n",
    "\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Enviorment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self, color, seed = 0):\n",
    "        self.env = gnwrapper.Animation(cr.CarRacing(color = color))\n",
    "        self.env = cr.CarRacing(color = color)\n",
    "        self.color = color\n",
    "        self.env.seed(seed)\n",
    "        self.reward_threshold = 1000\n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        self.av_r = self.reward_memory()\n",
    "\n",
    "        self.die = False\n",
    "        img_rgb = self.env.reset()\n",
    "        img_rgb = img_rgb / 128. - 1\n",
    "        self.stack = [img_rgb] * 4\n",
    "        return np.array(self.stack)\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        for i in range(8):\n",
    "            img_rgb, reward, die, _ = self.env.step(action)\n",
    "\n",
    "            if die: reward += 100\n",
    "\n",
    "            if self.color == 'g' and np.mean(img_rgb[:, :, 1]) > 185.0:\n",
    "                  reward -= 0.05\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            done = True if self.av_r(reward) <= -0.1 else False\n",
    "            if done or die:\n",
    "                break\n",
    "\n",
    "        img_rgb = img_rgb / 128. - 1\n",
    "        self.stack.pop(0)\n",
    "        self.stack.append(img_rgb)\n",
    "        assert len(self.stack) == 4\n",
    "        return np.array(self.stack), total_reward, done, die\n",
    "\n",
    "    def step_eval(self, action):\n",
    "        img_rgb, reward, done, _ = self.env.step(action)\n",
    "\n",
    "        img_rgb = img_rgb / 128. - 1\n",
    "        self.stack.pop(0)\n",
    "        self.stack.append(img_rgb)\n",
    "        return np.array(self.stack), reward, done, _\n",
    "\n",
    "    def render(self, *arg):\n",
    "        self.env.render(*arg)\n",
    "\n",
    "    @staticmethod\n",
    "    def reward_memory():\n",
    "        count = 0\n",
    "        length = 100\n",
    "        history = np.zeros(length)\n",
    "\n",
    "        def memory(reward):\n",
    "            nonlocal count\n",
    "            history[count] = reward\n",
    "            count = (count + 1) % length\n",
    "            return np.mean(history)\n",
    "\n",
    "        return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random(state):\n",
    "    red_scale, green_scale, blue_scale = 1., 1., 1.\n",
    "    base_scale = 0.5\n",
    "    while (red_scale == green_scale == blue_scale):\n",
    "        add_green = random.randint(0, 1)\n",
    "        add_red = random.randint(0, 1)\n",
    "        add_blue = random.randint(0, 1)\n",
    "        if (add_red): \n",
    "            red_scale = random.uniform(0.5, 1.1)\n",
    "        if (add_green): \n",
    "            green_scale = random.uniform(0.5, 1.1)\n",
    "        if (add_blue): \n",
    "            blue_scale = random.uniform(0.5, 1.1)\n",
    "            \n",
    "    for i in range (0, 4):\n",
    "        s = torch.transpose(state[i], 0, 2)\n",
    "        road = s [1] - s [0] * 0.6 - s[2] * 0.4\n",
    "        road = torch.stack((road, road, road), 0)\n",
    "        ones = torch.ones(3, 96, 96).to(device)\n",
    "\n",
    "        road_mask = torch.logical_xor(road, ones)\n",
    "        road_layer = s * road_mask\n",
    "\n",
    "        light_green = 204 / 128. - 1\n",
    "        light_green_mask = torch.logical_xor(s - light_green , ones)\n",
    "        light_green_layer = s * light_green_mask\n",
    "\n",
    "        dark_green = 230 / 128. - 1\n",
    "        dark_green_mask = torch.logical_xor(s - dark_green , ones)\n",
    "        dark_green_layer = s * dark_green_mask\n",
    "\n",
    "        bg_layer = light_green_layer + dark_green_layer\n",
    "\n",
    "        ones = torch.ones(96, 96).to(device)\n",
    "        back_ground_mask = torch.logical_xor(bg_layer[1], ones)\n",
    "\n",
    "        red_channel = (back_ground_mask * 128) /128. - 1\n",
    "        green_channel = (back_ground_mask * 128) /128. - 1\n",
    "        blue_channel = (back_ground_mask * 128) /128. - 1\n",
    "\n",
    "        if (add_red): red_channel = bg_layer[1] * red_scale\n",
    "        if (add_green): green_channel = bg_layer[1] * green_scale\n",
    "        if (add_blue): blue_channel = bg_layer[1] * blue_scale\n",
    "\n",
    "        new_bg_layer = torch.stack((red_channel, green_channel, blue_channel), 0)\n",
    "\n",
    "        new_state = new_bg_layer + road_layer\n",
    "\n",
    "        state[i] = torch.transpose(new_state, 0, 2)\n",
    "        \n",
    "    return state  \n",
    "\n",
    "def get_random_buffer(buffer, batch_size):\n",
    "    target_buffer = buffer.clone()\n",
    "    for i in range (batch_size):\n",
    "        target_buffer[i] = get_random(target_buffer[i])\n",
    "    return target_buffer    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None  \n",
    "\n",
    "class DANN(nn.Module):\n",
    "    def __init__(self, num_out = 2):\n",
    "        super(DANN, self).__init__()\n",
    "        self.sketch = nn.Sequential(\n",
    "            nn.Conv3d(4, 4, kernel_size=(1, 1, 3), stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.feature = nn.Sequential(  # input shape (4, 96, 96)\n",
    "            nn.Conv2d(4, 8, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2),  # (8, 47, 47)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2),  # (16, 23, 23)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2),  # (32, 11, 11)\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.cnn_base = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1),  # (64, 5, 5)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1),  # (128, 3, 3)\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 5 * 5, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1000, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100, 2),\n",
    "        )\n",
    "\n",
    "        self.v = nn.Sequential(nn.Linear(256, 100), nn.ReLU(), nn.Linear(100, 1))\n",
    "        self.fc = nn.Sequential(nn.Linear(256, 100), nn.ReLU())\n",
    "        self.alpha_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n",
    "        self.beta_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n",
    "        self.apply(self._weights_init)\n",
    "      \n",
    "    def forward(self, input, a = 0.1):\n",
    "        sketch = self.sketch(input)\n",
    "        sketch = torch.squeeze(sketch)\n",
    "\n",
    "        feature = self.feature(sketch)\n",
    "\n",
    "        out = self.cnn_base(feature)\n",
    "        out = out.view(-1, 256)\n",
    "        v = self.v(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        alpha = self.alpha_head(out) + 1\n",
    "        beta = self.beta_head(out) + 1\n",
    "\n",
    "        feature = feature.view(-1, 64 * 5 * 5)\n",
    "        reverse_feature = ReverseLayerF.apply(feature, a)\n",
    "        domain_output = self.domain_classifier(reverse_feature)\n",
    "\n",
    "        return (alpha, beta), v, domain_output, sketch\n",
    "\n",
    "    @staticmethod\n",
    "    def _weights_init(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.constant_(m.bias, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    max_grad_norm = 0.5\n",
    "    clip_param = 0.1\n",
    "    ppo_epoch = 10\n",
    "\n",
    "    transition = np.dtype([('s', np.float64, (4, 96, 96, 3)), ('a', np.float64, (3,)), ('a_logp', np.float64),\n",
    "                       ('r', np.float64), ('s_', np.float64, (4, 96, 96, 3))])\n",
    "    def __init__(self, net, criterion, optimizer, buffer_capacity = 2000, batch_size = 128):\n",
    "        self.net = net\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.source_buffer = np.empty(self.buffer_capacity, dtype=self.transition)\n",
    "        self.counter = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).double().to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            out = self.net.sketch(state)\n",
    "            out = torch.squeeze(out)\n",
    "            out = self.net.feature(out)\n",
    "\n",
    "            out = self.net.cnn_base(out)\n",
    "            out = out.view(-1, 256)\n",
    "            out = self.net.fc(out)\n",
    "            alpha = self.net.alpha_head(out) + 1\n",
    "            beta = self.net.beta_head(out) + 1\n",
    "\n",
    "        dist = Beta(alpha, beta)\n",
    "        action = dist.sample()\n",
    "        a_logp = dist.log_prob(action).sum(dim=1)\n",
    "\n",
    "        action = action.squeeze().cpu().numpy()\n",
    "        a_logp = a_logp.item()\n",
    "        return action, a_logp\n",
    "\n",
    "    def store(self, transition):\n",
    "        self.source_buffer[self.counter] = transition\n",
    "        self.counter += 1\n",
    "        if self.counter == self.buffer_capacity:\n",
    "            self.counter = 0\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def update(self, epoch, eta = 0.1):\n",
    "        s = torch.tensor(self.source_buffer['s'], dtype=torch.double).to(device)\n",
    "        a = torch.tensor(self.source_buffer['a'], dtype=torch.double).to(device)\n",
    "        r = torch.tensor(self.source_buffer['r'], dtype=torch.double).to(device).view(-1, 1)\n",
    "        s_ = torch.tensor(self.source_buffer['s_'], dtype=torch.double).to(device)\n",
    "        old_a_logp = torch.tensor(self.source_buffer['a_logp'], dtype=torch.double).to(device).view(-1, 1)\n",
    "\n",
    "        source_domain_label = torch.zeros(self.batch_size).long()\n",
    "        target_domain_label = torch.ones(self.batch_size).long()\n",
    "        source_domain_label = source_domain_label.to(device)\n",
    "        target_domain_label = target_domain_label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_v = r + 0.99 * self.net(s_)[1]\n",
    "            adv = target_v - self.net(s)[1]\n",
    "\n",
    "        image_array = []\n",
    "        for _ in range(self.ppo_epoch):\n",
    "            total = 0\n",
    "            source_domain_correct = 0\n",
    "            tagret_domain_correct = 0\n",
    "            add_image = True\n",
    "            \n",
    "            for index in BatchSampler(SubsetRandomSampler(range(self.buffer_capacity)), self.batch_size, True):\n",
    "                total += self.batch_size\n",
    "                loss = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    target_batch = get_random_buffer(s[index], self.batch_size)\n",
    "                \n",
    "\n",
    "                (alpha, beta), v, domain_out, s_sketch = self.net(s[index], eta)\n",
    "                source_domain_loss = self.criterion(domain_out, source_domain_label)\n",
    "\n",
    "                _, predicted = torch.max(domain_out.data, 1)\n",
    "                source_domain_correct += predicted.eq(source_domain_label.data).cpu().sum().item()\n",
    "\n",
    "                _, _,  domain_out, t_sketch = self.net(target_batch, eta)\n",
    "                target_domain_loss = self.criterion(domain_out, target_domain_label)\n",
    "\n",
    "                _, predicted = torch.max(domain_out.data, 1)\n",
    "                tagret_domain_correct += predicted.eq(target_domain_label.data).cpu().sum().item()\n",
    "                \n",
    "                if (add_image):\n",
    "                    image = s_sketch.reshape(96, 96)\n",
    "                    image = image.cpu().detach().numpy()\n",
    "                    image_array.append(image)\n",
    "                \n",
    "                    image = t_sketch.reshape(96, 96)\n",
    "                    image = image.cpu().detach().numpy()\n",
    "                    image_array.append(image)\n",
    "                    \n",
    "                    add_image = False\n",
    "\n",
    "                dist = Beta(alpha, beta)\n",
    "                a_logp = dist.log_prob(a[index]).sum(dim=1, keepdim=True)\n",
    "                ratio = torch.exp(a_logp - old_a_logp[index])\n",
    "                surr1 = ratio * adv[index]\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * adv[index]\n",
    "                action_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = F.smooth_l1_loss(self.net(s[index])[1], target_v[index])\n",
    "\n",
    "                loss += action_loss + 2. * value_loss + source_domain_loss + target_domain_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            print (\"Source Correct: %.2f, Target Correct: %.2f\" % (source_domain_correct / total, \\\n",
    "                                                                   tagret_domain_correct / total))\n",
    "        f, axs = plt.subplots(2, 10, figsize = (16, 4))\n",
    "        for img, ax in zip(image_array, axs):\n",
    "            ax.imshow(img)\n",
    "        f.savefig('./output/%04d.png' % epoch)\n",
    "        plt.close(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(agent, env):\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for t in range(1000):\n",
    "        action, a_logp = agent.select_action(state)\n",
    "\n",
    "        state_, reward, done, _ = env.step_eval(action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        state = state_\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(source_env, target_env, agent):\n",
    "    training_records= []\n",
    "    running_score_records = []\n",
    "    running_score = 0\n",
    "    \n",
    "    c1_running_score = 0\n",
    "    c1_training_records = []\n",
    "    c1_running_score_records = []\n",
    "    \n",
    "    c2_running_score = 0\n",
    "    c2_training_records = []\n",
    "    c2_running_score_records = []\n",
    "    \n",
    "    for i_ep in range(3000):\n",
    "        score = 0\n",
    "        state = source_env.reset()\n",
    "\n",
    "        if (i_ep < 500): eta = 0.8\n",
    "        else: eta = 0.1\n",
    "\n",
    "        for t in range(1000):\n",
    "            action, a_logp = agent.select_action(state)\n",
    "            state_, reward, done, die = source_env.step(action * np.array([2., 1., 1.])\\\n",
    "                                                                       + np.array([-1., 0., 0.]))\n",
    "            score += reward\n",
    "\n",
    "            should_update = agent.store((state, action, a_logp, reward, state_))\n",
    "\n",
    "            if should_update: \n",
    "                agent.update(epoch = i_ep, eta = eta)\n",
    "\n",
    "            state = state_\n",
    "\n",
    "            if done or die: break\n",
    "                \n",
    "        training_records.append(score)\n",
    "        running_score = running_score * 0.99 + score * 0.01\n",
    "        running_score_records.append(running_score)\n",
    "        \n",
    "        c1_score = eval(agent, target_env[0])\n",
    "        c2_score = eval(agent, target_env[1])\n",
    "        c1_training_records.append(c1_score)\n",
    "        c2_training_records.append(c2_score)\n",
    "        \n",
    "        c1_running_score = c1_running_score * 0.99 + c1_score * 0.01\n",
    "        c2_running_score = c2_running_score * 0.99 + c2_score * 0.01\n",
    "        c1_running_score_records.append(c1_running_score)\n",
    "        c2_running_score_records.append(c2_running_score)\n",
    "        \n",
    "        if (i_ep % 10 == 0):\n",
    "            print('Ep {}\\tLast score: {:.2f}\\tMoving average score: {:.2f}'.format(i_ep, score, running_score))\n",
    "            print('c1 score: {:.2f}\\t c1 Moving average: {:.2f}'.format(c1_score, c1_running_score))\n",
    "            print('c2 score: {:.2f}\\t c2 Moving average: {:.2f}'.format(c2_score, c2_running_score))\n",
    "            \n",
    "            if (i_ep != 0):\n",
    "                plt.plot(range(len(training_records)), training_records, \"-b\")\n",
    "                plt.plot(range(len(running_score_records)), running_score_records, \"-r\")\n",
    "                plt.plot(range(len(c1_training_records)), c1_training_records, \"-b\")\n",
    "                plt.plot(range(len(c1_running_score_records)), c1_running_score_records)\n",
    "                plt.plot(range(len(c2_training_records)), c2_training_records)\n",
    "                plt.plot(range(len(c2_running_score_records)), c2_running_score_records)\n",
    "                plt.legend(['s', 's r', 'c1', 'c1 r', 'c2', 'c2 r'])\n",
    "                plt.savefig('./output/result_%04d.png' % i_ep)\n",
    "                plt.close(f)\n",
    "        \n",
    "        if (c1_running_score > 850 and c2_running_score > 850):\n",
    "            break\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0\tLast score: -38.06\tMoving average score: -0.38\n",
      "c1 score: -22.82\t c1 Moving average: -0.23\n",
      "c2 score: -22.82\t c2 Moving average: -0.23\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Ep 10\tLast score: -7.97\tMoving average score: -2.09\n",
      "c1 score: -9.45\t c1 Moving average: -2.62\n",
      "c2 score: -9.45\t c2 Moving average: -2.63\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n"
     ]
    }
   ],
   "source": [
    "green_env = Env(color = 'g', seed = 0)\n",
    "env_c1 = Env(color = 'c1', seed = 0)\n",
    "env_c2 = Env(color = 'c2', seed = 0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "net = DANN(num_out = 2).double().cuda()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "agent = Agent(net = net,  criterion = criterion,  optimizer = optimizer, buffer_capacity = 3000, batch_size = 64)\n",
    "\n",
    "source_env = green_env\n",
    "target_env = [env_c1, env_c2]\n",
    "green_record, green_running_scores = train(green_env, target_env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
