{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "from torch.distributions import Beta\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "from torch.autograd import Function\n",
    "from torch.utils.data import SubsetRandomSampler, BatchSampler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import env\n",
    "import DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "if (os.path.exists(\"./output_r\")) == False:\n",
    "    os.mkdir(\"output_r\")\n",
    "    \n",
    "for epoch in range (3000):\n",
    "    files = glob.glob(\"./output_r/*.png\")\n",
    "    \n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviorment Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_env = env.Env(color = 'g', seed = 0)\n",
    "unseen_1_env = env.Env(color = 'c1', seed = 0)\n",
    "unseen_2_env = env.Env(color = 'c2', seed = 0)\n",
    "\n",
    "discrete_actions = {0 : np.array([0,0,0]),       # do nothing\n",
    "                                 1 : np.array([-1,0,0]),      # steer sharp left\n",
    "                                 2 : np.array([1,0,0]),       # steer sharp right\n",
    "                                 3 : np.array([-0.5,0,0]),    # steer left\n",
    "                                 4 : np.array([0.5,0,0]),     # steer right\n",
    "                                 5 : np.array([0,1,0]),       # accelerate 100%\n",
    "                                 6 : np.array([0,0.5,0]),     # accelerate 50%\n",
    "                                 7 : np.array([0,0.25,0]),    # accelerate 25%\n",
    "                                 8 : np.array([0,0,1]),       # brake 100%\n",
    "                                 9 : np.array([0,0,0.5]),     # brake 50%\n",
    "                                 10 : np.array([0,0,0.25])}   # brake 25%\n",
    "\n",
    "def get_obs(env):\n",
    "    n_actions = len(discrete_actions)\n",
    "    for i in range (0, 30):\n",
    "        action = torch.randint(low=0, high=11, size=(1,))\n",
    "        action_transfered = discrete_actions.get(int(action[0]))\n",
    "\n",
    "        obs, reward, done, _ = env.step([action_transfered[0], action_transfered[1], action_transfered[2]])\n",
    "\n",
    "    return (obs[0] + 1) / 2.0\n",
    "\n",
    "env_preview = []\n",
    "\n",
    "env_preview.append(get_obs(source_env))\n",
    "env_preview.append(get_obs(unseen_1_env))\n",
    "env_preview.append(get_obs(unseen_2_env))\n",
    "\n",
    "\n",
    "f, axs = plt.subplots(1, 3, figsize = (12, 4))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for img, ax in zip(env_preview, axs):\n",
    "    ax.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DANN(num_out = 2).double().cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Random Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random(state):\n",
    "    red_scale, green_scale, blue_scale = 1., 1., 1.\n",
    "    base_scale = 0.5\n",
    "    while (red_scale == green_scale == blue_scale):\n",
    "        add_green = random.randint(0, 1)\n",
    "        add_red = random.randint(0, 1)\n",
    "        add_blue = random.randint(0, 1)\n",
    "        if (add_red): \n",
    "            red_scale = random.uniform(0.5, 1.1)\n",
    "        if (add_green): \n",
    "            green_scale = random.uniform(0.5, 1.1)\n",
    "        if (add_blue): \n",
    "            blue_scale = random.uniform(0.5, 1.1)\n",
    "            \n",
    "    for i in range (0, 4):\n",
    "        s = torch.transpose(state[i], 0, 2)\n",
    "        road = s [1] - s [0] * 0.6 - s[2] * 0.4\n",
    "        road = torch.stack((road, road, road), 0)\n",
    "        ones = torch.ones(3, 96, 96).to(device)\n",
    "\n",
    "        road_mask = torch.logical_xor(road, ones)\n",
    "        road_layer = s * road_mask\n",
    "\n",
    "        light_green = 204 / 128. - 1\n",
    "        light_green_mask = torch.logical_xor(s - light_green , ones)\n",
    "        light_green_layer = s * light_green_mask\n",
    "\n",
    "        dark_green = 230 / 128. - 1\n",
    "        dark_green_mask = torch.logical_xor(s - dark_green , ones)\n",
    "        dark_green_layer = s * dark_green_mask\n",
    "\n",
    "        bg_layer = light_green_layer + dark_green_layer\n",
    "\n",
    "        ones = torch.ones(96, 96).to(device)\n",
    "        back_ground_mask = torch.logical_xor(bg_layer[1], ones)\n",
    "\n",
    "        red_channel = (back_ground_mask * 128) /128. - 1\n",
    "        green_channel = (back_ground_mask * 128) /128. - 1\n",
    "        blue_channel = (back_ground_mask * 128) /128. - 1\n",
    "\n",
    "        if (add_red): red_channel = bg_layer[1] * red_scale\n",
    "        if (add_green): green_channel = bg_layer[1] * green_scale\n",
    "        if (add_blue): blue_channel = bg_layer[1] * blue_scale\n",
    "\n",
    "        new_bg_layer = torch.stack((red_channel, green_channel, blue_channel), 0)\n",
    "\n",
    "        new_state = new_bg_layer + road_layer\n",
    "\n",
    "        state[i] = torch.transpose(new_state, 0, 2)\n",
    "        \n",
    "    return state  \n",
    "\n",
    "def get_random_buffer(buffer, batch_size):\n",
    "    target_buffer = buffer.clone()\n",
    "    for i in range (batch_size):\n",
    "        target_buffer[i] = get_random(target_buffer[i])\n",
    "    return target_buffer    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    max_grad_norm = 0.5\n",
    "    clip_param = 0.1\n",
    "    ppo_epoch = 10\n",
    "\n",
    "    transition = np.dtype([\n",
    "        ('s', np.float64, (4, 96, 96, 3)),\n",
    "        ('a', np.float64, (3,)),\n",
    "        ('a_logp', np.float64),\n",
    "        ('r', np.float64),\n",
    "        ('s_', np.float64, (4, 96, 96, 3))\n",
    "    ])\n",
    "\n",
    "    def __init__(self, net, criterion, optimizer, buffer_capacity=2000, batch_size=128):\n",
    "        \"\"\"\n",
    "        Initialize the Agent.\n",
    "        \n",
    "        :param net: neural network for the agent\n",
    "        :param criterion: loss function\n",
    "        :param optimizer: optimizer for the network\n",
    "        :param buffer_capacity: int, capacity of the buffer\n",
    "        :param batch_size: int, batch size for training\n",
    "        \"\"\"\n",
    "        self.net = net\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.source_buffer = np.empty(self.buffer_capacity, dtype=self.transition)\n",
    "        self.counter = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select action based on the given state.\n",
    "        \n",
    "        :param state: numpy array, state representation\n",
    "        :return: tuple, (action, action_log_probability)\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).double().to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            out = self.net.sketch(state)\n",
    "            out = torch.squeeze(out)\n",
    "            out = self.net.feature(out)\n",
    "\n",
    "            out = self.net.cnn_base(out)\n",
    "            out = out.view(-1, 256)\n",
    "            out = self.net.fc(out)\n",
    "            alpha = self.net.alpha_head(out) + 1\n",
    "            beta = self.net.beta_head(out) + 1\n",
    "\n",
    "        dist = Beta(alpha, beta)\n",
    "        action = dist.sample()\n",
    "        a_logp = dist.log_prob(action).sum(dim=1)\n",
    "\n",
    "        action = action.squeeze().cpu().numpy()\n",
    "        a_logp = a_logp.item()\n",
    "        return action, a_logp\n",
    "\n",
    "    def store(self, transition):\n",
    "        \"\"\"\n",
    "        Store the transition in the buffer.\n",
    "        \n",
    "        :param transition: tuple, (state, action, action_log_probability, reward, next_state)\n",
    "        :return: bool, True if the buffer is full, False otherwise\n",
    "        \"\"\"\n",
    "        self.source_buffer[self.counter] = transition\n",
    "        self.counter += 1\n",
    "        if self.counter == self.buffer_capacity:\n",
    "            self.counter = 0\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _prepare_tensors(self):\n",
    "        s = torch.tensor(self.source_buffer['s'], dtype=torch.double).to(device)\n",
    "        a = torch.tensor(self.source_buffer['a'], dtype=torch.double).to(device)\n",
    "        r = torch.tensor(self.source_buffer['r'], dtype=torch.double).to(device).view(-1, 1)\n",
    "        s_ = torch.tensor(self.source_buffer['s_'], dtype=torch.double).to(device)\n",
    "        old_a_logp = torch.tensor(self.source_buffer['a_logp'], dtype=torch.double).to(device).view(-1, 1)\n",
    "\n",
    "        return s, a, r, s_, old_a_logp\n",
    "    \n",
    "    def _prepare_domain_labels(self):\n",
    "        source_domain_label = torch.zeros(self.batch_size).long().to(device)\n",
    "        target_domain_label = torch.ones(self.batch_size).long().to(device)\n",
    "        return source_domain_label, target_domain_label\n",
    "\n",
    "    def _compute_advantage(self, s, r, s_):\n",
    "        with torch.no_grad():\n",
    "            target_v = r + 0.99 * self.net(s_)[1]\n",
    "            adv = target_v - self.net(s)[1]\n",
    "        return target_v, adv\n",
    "\n",
    "    def _calculate_losses(self, s, a, old_a_logp, adv, index, eta):\n",
    "        (alpha, beta), v, domain_out, s_sketch = self.net(s[index], eta)\n",
    "\n",
    "        source_domain_loss = self.criterion(domain_out, self.source_domain_label)\n",
    "        source_domain_correct = (torch.argmax(domain_out, dim=1) == self.source_domain_label).sum().item()\n",
    "\n",
    "        _, _, domain_out, t_sketch = self.net(self.target_batch, eta)\n",
    "        target_domain_loss = self.criterion(domain_out, self.target_domain_label)\n",
    "        target_domain_correct = (torch.argmax(domain_out, dim=1) == self.target_domain_label).sum().item()\n",
    "\n",
    "        dist = Beta(alpha, beta)\n",
    "        a_logp = dist.log_prob(a[index]).sum(dim=1, keepdim=True)\n",
    "        ratio = torch.exp(a_logp - old_a_logp[index])\n",
    "        surr1 = ratio * adv[index]\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * adv[index]\n",
    "        action_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = F.smooth_l1_loss(self.net(s[index])[1], self.target_v[index])\n",
    "\n",
    "        loss = action_loss + 2. * value_loss + source_domain_loss + target_domain_loss\n",
    "\n",
    "        return loss, source_domain_correct, target_domain_correct, s_sketch, t_sketch\n",
    "\n",
    "    def _save_images(self, epoch, image_array):\n",
    "        f, axs = plt.subplots(2, 10, figsize = (16, 4))\n",
    "        axs = axs.flatten()\n",
    "        for img, ax in zip(image_array, axs):\n",
    "            ax.imshow(img)\n",
    "        f.savefig('./output_r/%04d.png' % epoch)\n",
    "        plt.close(f)\n",
    "\n",
    "\n",
    "    def update(self, epoch, eta=0.1):\n",
    "        # Prepare tensors from the source_buffer\n",
    "        s, a, r, s_, old_a_logp = self._prepare_tensors()\n",
    "        self.target_v, adv = self._compute_advantage(s, r, s_)\n",
    "        self.source_domain_label, self.target_domain_label = self._prepare_domain_labels()\n",
    "\n",
    "        image_array, source_acc_array, target_acc_array = [], [], []\n",
    "\n",
    "        for _ in range(self.ppo_epoch):\n",
    "            total = 0\n",
    "            source_domain_correct, target_domain_correct = 0, 0\n",
    "            add_image = True\n",
    "\n",
    "            for index in BatchSampler(SubsetRandomSampler(range(self.buffer_capacity)), self.batch_size, True):\n",
    "                total += self.batch_size\n",
    "                self.target_batch = get_random_buffer(s[index], self.batch_size)\n",
    "\n",
    "                loss, src_correct, tgt_correct, s_sketch, t_sketch = self._calculate_losses(s, a, old_a_logp, adv, index, eta)\n",
    "                source_domain_correct += src_correct\n",
    "                target_domain_correct += tgt_correct\n",
    "\n",
    "                if add_image:\n",
    "                    image_array.extend([s_sketch[0][0].reshape(96, 96).cpu().detach().numpy(),\n",
    "                                        t_sketch[0][0].reshape(96, 96).cpu().detach().numpy()])\n",
    "                    add_image = False\n",
    "\n",
    "                self._update_network(loss)\n",
    "\n",
    "            source_acc_array.append(source_domain_correct / total)\n",
    "            target_acc_array.append(target_domain_correct / total)\n",
    "\n",
    "        mean_source_acc = np.mean(source_acc_array)\n",
    "        mean_target_acc = np.mean(target_acc_array)\n",
    "\n",
    "        self._save_images(epoch, image_array)\n",
    "\n",
    "        return mean_source_acc, mean_target_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(agent, env):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the given environment.\n",
    "\n",
    "    :param agent: Agent, trained agent to be evaluated\n",
    "    :param env: Environment, environment to evaluate the agent on\n",
    "    :return: float, total reward received by the agent\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for t in range(1000):\n",
    "        action, a_logp = agent.select_action(state)\n",
    "        state_, reward, done, _ = env.step_eval(action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n",
    "        score += reward\n",
    "        state = state_\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return score\n",
    "\n",
    "def train(source_env, target_env, agent):\n",
    "    \"\"\"\n",
    "    Train the agent on the source environment and evaluate it on the target environments.\n",
    "\n",
    "    :param source_env: Environment, source environment for training the agent\n",
    "    :param target_env: list, target environments for evaluating the agent\n",
    "    :param agent: Agent, reinforcement learning agent\n",
    "    \"\"\"\n",
    "    training_records = []\n",
    "    running_score_records = []\n",
    "    running_score = 0\n",
    "\n",
    "    c1_running_score = 0\n",
    "    c1_training_records = []\n",
    "    c1_running_score_records = []\n",
    "\n",
    "    c2_running_score = 0\n",
    "    c2_training_records = []\n",
    "    c2_running_score_records = []\n",
    "\n",
    "    eta = 0.2\n",
    "\n",
    "    for i_ep in range(3000):\n",
    "        score = 0\n",
    "        state = source_env.reset()\n",
    "\n",
    "        for t in range(1000):\n",
    "            action, a_logp = agent.select_action(state)\n",
    "            state_, reward, done, die = source_env.step(action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n",
    "            score += reward\n",
    "\n",
    "            should_update = agent.store((state, action, a_logp, reward, state_))\n",
    "\n",
    "            if should_update:\n",
    "                eta_max = 0.5 if i_ep < 500 else (0.45 if i_ep < 1500 else 0.3)\n",
    "                print(\"eta: {:.2f}\".format(eta))\n",
    "                s_acc, t_acc = agent.update(epoch=i_ep, eta=eta)\n",
    "                eta = 0.1\n",
    "\n",
    "            state = state_\n",
    "\n",
    "            if done or die:\n",
    "                break\n",
    "\n",
    "        # Record scores and calculate moving averages\n",
    "        training_records.append(score)\n",
    "        running_score = running_score * 0.99 + score * 0.01\n",
    "        running_score_records.append(running_score)\n",
    "\n",
    "        c1_score = eval(agent, target_env[0])\n",
    "        c2_score = eval(agent, target_env[1])\n",
    "        c1_training_records.append(c1_score)\n",
    "        c2_training_records.append(c2_score)\n",
    "\n",
    "        c1_running_score = c1_running_score * 0.99 + c1_score * 0.01\n",
    "        c2_running_score = c2_running_score * 0.99 + c2_score * 0.01\n",
    "        c1_running_score_records.append(c1_running_score)\n",
    "        c2_running_score_records.append(c2_running_score)\n",
    "\n",
    "        # Display progress every 10 episodes\n",
    "        if i_ep + 1 % 10 == 0:\n",
    "            print('Ep {}\\tLast score: {:.2f}\\tMoving average score: {:.2f}'.format(i_ep, score, running_score))\n",
    "            print('c1 score: {:.2f}\\t c1 Moving average: {:.2f}'.format(c1_score, c1_running_score))\n",
    "            print('c2 score: {:.2f}\\t c2 Moving average: {:.2f}'.format(c2_score, c2_running_score))\n",
    "\n",
    "            f, axs = plt.subplots(1, 2, figsize = (16, 8))\n",
    "            axs[0].plot(range(len(training_records)), training_records)\n",
    "            axs[0].plot(range(len(c1_training_records)), c1_training_records)\n",
    "            axs[0].plot(range(len(c2_training_records)), c2_training_records)\n",
    "                \n",
    "            axs[1].plot(range(len(running_score_records)), running_score_records)\n",
    "            axs[1].plot(range(len(c1_running_score_records)), c1_running_score_records)\n",
    "            axs[1].plot(range(len(c2_running_score_records)), c2_running_score_records)\n",
    "                \n",
    "            axs[0].legend(['s', 'c1', 'c2'])\n",
    "            axs[1].legend(['s', 'c1', 'c2'])\n",
    "\n",
    "            f.savefig('./output_r/result_%04d.png' % i_ep)\n",
    "            plt.close(f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
